! Character Encoding and Resource Meta Description

The rise of the Internet and of Open Standards resulted in the adoption of a number of fundamental mechanisms to enable communication and collaboration between different systems.

One such mechanism is the ability to encode strings or characters to bytes or to decode strings or characters from bytes. Different encoding standards have been developed over the years. Pharo supports many current and legacy encodings.

Another important aspect is the ability to describe resources such as files. Both Mime-Type and URLs or URIs are basic building blocks for creating meta descriptions of resources. Pharo has objects that implement these fundamental aspects.

Character encoding, MIME types and URL/URIs are essential for the correct implementation of HTTP, but they are indepent of it, as they are used for many other purposes.   


!! Character encoding

!!! Characters and Strings use Unicode internally

Proper character encoding and decoding is crucial in today's international world. Internally, Pharo stores characters and strings using Unicode. *Unicode>http://en.wikipedia.org/wiki/Unicode* is a very large internationally standardized collection of code points, integers, representing all of the world languages' characters.

Let's look at some example strings with their Unicode code points:

[[[
'Hello' collect: #codePoint as: Array. 

  => #(72 101 108 108 111)

'Les élèves français' collect: #codePoint as: Array. 

  => #(76 101 115 32 233 108 232 118 101 115 
       32 102 114 97 110 231 97 105 115)

'Ελλάδα' collect: #codePoint as: Array. 

  => #(917 955 955 940 948 945)
]]]

For a simple language like English, all characters have code points below 128 (these code points can even fit in 7 bits). These characters are part of *ASCII>http://en.wikipedia.org/wiki/ASCII*. The very first part of the so called Basic Multilingual Plane of Unicode (the first 128 code points of it) are identical to ASCII.

Next come a number of European languages, like French, which have code points below 256 (fitting in 8 bits or one byte). These characters are part of *Latin-1 (ISO-8859-1)>http://en.wikipedia.org/wiki/ISO/IEC_8859-1*, whose first 256 code points are identical in Unicode.

And finally, there are hundreds of other languages, like Chinese, Japanse, Cyrillic, Arabic or Greek. You can see from the example (Greece in Greek) that those code points are higher than 256 and thus no longer fit in one byte.

The good thing is, we can work with text in any language in Pharo. To display everything correctly you have to use a font that is capable of showing all the characters (or glyphs) needed. Arial Unicode MS is such a font.

!!! Encoding and Decoding

For communication with the world outside Pharo, the operating system, files, the internet, we have to represent our strings as a collection of bytes.

Character encoding is standard way of converting a native Pharo string, a collection of Unicode code points to series of bytes. Character decoding is the reverse process: interpreting a series of bytes as a collection of Unicode code points, as a Pharo string.

To implement character encoding or decoding, a concrete ==ZnCharacterEncoder== subclass like ==ZnUTF8Encoder== is used. Character encoders do the following:

- encode a character (==#nextPut:toStream:==) or string (==#next:putAll:startingAt:toStream:==) onto a binary stream
- convert a string (==#encodeString:==) to a byte array
- decode a binary stream to a character (==#nextFromStream:==) or string (==#readInto:startingAt:count:fromStream:==)
- convert a byte array to string (==#decodeBytes:==)
- compute the number of bytes needed to encode a character (==#encodedByteCountFor:==) or string (==#encodedByteCountForString:==)
- move a binary stream backwards one character (==#backOnStream:==)

Character encoders do proper error handling, throwing ==ZnCharacterEncodingError== when something goes wrong. The strict/lenient setting controls some behavior in this respect.

The primary internet encoding is *UTF-8>http://en.wikipedia.org/wiki/UTF-8*. This is also the recommended encoding. UTF-8 is a variable length encoding that is optimized somewhat for ASCII and to a lesser degree Latin1 and some other common European encodings. 

!!! Converting Strings and ByteArrays

You will deal only indirectly with character encoders. String and ByteArray have some convenience methods to do encoding and decoding. Let's have a look.

[[[
'Hello' utf8Encoded. 
  => #[72 101 108 108 111]

'Hello' encodeWith: #latin1.
  => #[72 101 108 108 111]

'Les élèves français' utf8Encoded. 
  => #[76 101 115 32 195 169 108 195 168 118 101 115 
       32 102 114 97 110 195 167 97 105 115]

'Les élèves français' encodeWith: #latin1. 
  => #[76 101 115 32 233 108 232 118 101 115 
       32 102 114 97 110 231 97 105 115]

'Ελλάδα' utf8Encoded. 
  => #[206 149 206 187 206 187 206 172 206 180 206 177]

'Ελλάδα' encodeWith: #latin1.
  => ZnCharacterEncodingError: 'Character Unicode code point outside encoder range'
]]]

Our ASCII string, 'Hello' encodes identically using either UTF-8 or Latin-1. Our French string, 'Les élèves français', encodes differently though. The reason is that UTF-8 uses two bytes for the accented letters like é, è and ç. Our greek string, 'Ελλάδα', even gives an error when we try to encode it using Latin-1. The reason is that the Greek letters are outside of the alphabet of Latin-1. Still, UTF-8 manages to encode them using just two bytes.

The reverse process, decoding, is equally simple.

[[[
#[72 101 108 108 111] utf8Decoded.
  => 'Hello'

#[72 101 108 108 111] decodeWith: #latin1.
  => 'Hello'

#[76 101 115 32 195 169 108 195 168 118 101 115 
  32 102 114 97 110 195 167 97 105 115] utf8Decoded.
  => 'Les élèves français'

#[76 101 115 32 195 169 108 195 168 118 101 115 
  32 102 114 97 110 195 167 97 105 115] decodeWith: #latin1.
  => 'Les Ã©lÃ¨ves franÃ§ais'

#[76 101 115 32 233 108 232 118 101 115 
  32 102 114 97 110 231 97 105 115] utf8Decoded.
  => ZnInvalidUTF8: 'Illegal continuation byte for utf-8 encoding'

#[76 101 115 32 233 108 232 118 101 115 
  32 102 114 97 110 231 97 105 115] decodeWith: #latin1.
  => 'Les élèves français'

#[206 149 206 187 206 187 206 172 206 180 206 177] utf8Decoded.
  => 'Ελλάδα'

#[206 149 206 187 206 187 206 172 206 180 206 177] decodeWith: #latin1.
  => ZnCharacterEncodingError: 'Character Unicode code point outside encoder range'
]]]

Our English 'Hello', being pure ASCII, can be decoded using either UTF-8 or Latin-1. Our French 'Les élèves français' is another story: using the wrong encoding gives either gibberish or ==ZnInvalidUTF8== error. The same is true for our Greek 'Ελλάδα'. 

You might wonder why in the first case the #latin1 encoder produced gibberish, while in the second case it gave an error. This is because in the second case, there was a byte with value 149, that is outside its alphabet. So called byte encoders, like Latin-1, take a subset of Unicode characters and compress them in 256 possible byte values. If you are curious, you can inspect the character or byte domains of a ==ZnByteEncoder==.

[[[
(ZnByteEncoder newForEncoding: 'iso-8859-1') byteDomain.
(ZnByteEncoder newForEncoding: 'ISO_8859_7') characterDomain.
]]]

Note that identifiers for encodings are interpreted flexibly (case and punctuation do not matter).

There exists a special ==ZnNullEncoder== that basically does nothing: it treats bytes are characters and vice versa. This is actually mostly equivalent to Latin-1 or ISO-8859-1. And yes, that is a bit confusing.

!!! Converting Streams

The second primary use of encoders is when dealing with streams. More specifically, when interpreting a binary read or write stream as a character stream. Note that at their lowest level, all streams to and from the operating system or network are binary and thus need the use of an encoder when treating them as character streams.

To treat a binary write stream as a character write stream, you wrap is with a ==ZnCharacterWriteStream==. Similary, to treat a binary read stream as a character stream, you wrap it with a ==ZnCharacterReadStream==. Here are some examples.

[[[
ByteArray streamContents: [ :out |
  (ZnCharacterWriteStream on: out encoding: #iso88591)
     nextPutAll: 'Les élèves français' ].

#[76 101 115 32 233 108 232 118 101 115 
  32 102 114 97 110 231 97 105 115] readStream in: [ :in |
    (ZnCharacterReadStream on: in encoding: #iso8859) 
       upToEnd ].

'encoding-test.txt' asFileReference writeStreamDo: [ :out |
  (ZnCharacterWriteStream on: out binary encoding: #utf8)
     nextPutAll: 'Hello'; space; nextPutAll: 'Ελλάδα'; crlf;
	 nextPutAll: 'Les élèves français'; crlf ].

'encoding-test.txt' asFileReference readStreamDo: [ :in |
  (ZnCharacterReadStream on: in binary encoding: #utf8)
     upToEnd ]
]]]

We used ==#on:encoding:== here, but there is also a plain ==#on:== instance creation method that defaults to UTF-8. Internally, the character streams will use an encoder instance to do the actual work. 


%%%%%%%%%%%%%%%% MARKER %%%%%%%%%%%%%%%
!!! ----- MARKER -----

==ZnCharacterEncoding== is an extension and reimplementation of regular TextConverter. It only works on binary input and generated binary output. It adds the ability to compute the encoded length of a source character, a crucial operation for HTTP. It is more correct and will throw proper exceptions when things go wrong.

Character encoding is mostly invisible. Here are some code snippets using the encoders directly, feel free to substitute any Unicode character to make the test more interesting.

[[[
| encoder string |
encoder := ZnUTF8Encoder new.
string := 'any Unicode'.
self assert: (encoder decodeBytes: (encoder encodeString: string)) equals: string.
encoder encodedByteCountForString: string.
]]]

There are no automatic conversions in Zinc. So Zinc is one of the pieces of software that does not assume stupid defaults. You should specify a proper Content-Type header including the charset information. Otherwise Zinc has no chance of knowing what to use and the default NullEncoder will make your string wrong.

Let us look at one example 
SD: We should add back the ö umlaut whe pillar/latex can handle it.

[[[
ZnServer startDefaultOn: 1701.

ZnClient new
 url: 'http://localhost:1701/echo';
 entity: (ZnEntity with: 'An der schönen blauen Donau'); 
 post.
	
ZnClient new
 url: 'http://localhost:1701/echo';
 entity: (ZnEntity 
           with: 'An der schönen blauen Donau' 
           type: (ZnMimeType textPlain charSet: #'iso-8859-1'; yourself));
 post;
 yourself.
]]]

In the first case, a UTF-8 encoded string is POST-ed and correctly returned (in a UTF-8 encoded response).

In the second case, an ISO-8859-1 encoded string is POST-ed and correctly returned (in a UTF-8 encoded response).

In both cases the decoding was done correctly, using the specified charset (if that is missing, the ZnNullEncoder is used). Now, ö is not a perfect test example because its encoding value in Unicode, 246 decimal, U\+00F6 hex, still fits in 1 byte and hence survives null encoding/decoding. That is why the following still works, although it is wrong to drop the charset.

[[[
ZnClient new
 url: 'http://localhost:1701/echo';
 entity: (ZnEntity 
           with: 'An der schönen blauen Donau' 
           type: (ZnMimeType textPlain clearCharSet; yourself));
 post;
 yourself.
]]]


!! Mime-Types

A mime-type that is an official, cross-platform definition of a file or document type or format. Again, see the Wikepedia article *Internet media type>http://en.wikipedia.org/wiki/Mime-type* for more details.

Zn models mime-types using its ==ZnMimeType== object which has 3 components

- a main type, for example text or image, 
- a sub type, for example plain or html, or jpeg, png or gif, and 
- a number of attributes, for example ==charset=utf-8==.

The class side of ==ZnMimeType== has some convenience methods for accessing well known mime-types. Note that for textual (non-binary) types, the encoding defaults to UTF-8, the prevalent internet standard. Creating a ==ZnMimeType== object is as easy as sending ==asZnMimeType== to a ==String==.

% @@why should we invoke ZnMimeType textHtml.@@
[[[
ZnMimeType textHtml.
'text/html;charset=utf-8' asZnMimeType.
]]]

The subtype can be a wildcard, indicated by a ==*==. This allows for matching.

[[[
ZnMimeType textHtml matches: ZnMimeType text.
]]]
